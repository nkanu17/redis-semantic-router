{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage Analysis\n",
    "\n",
    "Analyzing train_data.csv and validation_data.csv for potential data leakage between training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the datasets - this will trigger auto-split from BBC News Train.csv if needed\nfrom pathlib import Path\nimport sys\nsys.path.append('src')\nfrom utils.data_loader import NewsDataLoader\n\n# Initialize data loader - this will auto-create the split files from BBC News Train.csv\ndata_loader = NewsDataLoader(\\\"data/bbc-news-articles-labeled\\\")\n\n# Now load the split datasets\ntrain_data = pd.read_csv(\\\"data/bbc-news-articles-labeled/train_data.csv\\\")\nvalidation_data = pd.read_csv(\\\"data/bbc-news-articles-labeled/validation_data.csv\\\")\n\nprint(\\\"Dataset shapes:\\\")\nprint(f\\\"Training set: {train_data.shape}\\\")\nprint(f\\\"Validation set: {validation_data.shape}\\\")\n\nprint(\\\"\\\\nColumn check:\\\")\nprint(f\\\"Train columns: {train_data.columns.tolist()}\\\")\nprint(f\\\"Validation columns: {validation_data.columns.tolist()}\\\")\n\nprint(\\\"\\\\nFirst few rows of training data:\\\")\ndisplay(train_data.head())\""
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['Text'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ DATA LEAKAGE ANALYSIS - ARTICLE IDs\n",
      "==================================================\n",
      "Training set ArticleIds: 1117\n",
      "Validation set ArticleIds: 373\n",
      "Overlapping ArticleIds: 0\n",
      "\n",
      "âœ… No duplicate ArticleIds found\n",
      "ID leakage percentage: 0.0% of validation set\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate ArticleIds between train and validation sets\n",
    "train_ids = set(train_data['ArticleId'])\n",
    "validation_ids = set(validation_data['ArticleId'])\n",
    "\n",
    "# Find overlapping IDs\n",
    "overlapping_ids = train_ids.intersection(validation_ids)\n",
    "\n",
    "print(\"ðŸš¨ DATA LEAKAGE ANALYSIS - ARTICLE IDs\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set ArticleIds: {len(train_ids)}\")\n",
    "print(f\"Validation set ArticleIds: {len(validation_ids)}\")\n",
    "print(f\"Overlapping ArticleIds: {len(overlapping_ids)}\")\n",
    "\n",
    "if overlapping_ids:\n",
    "    print(f\"\\nâš ï¸  CRITICAL: {len(overlapping_ids)} duplicate ArticleIds found!\")\n",
    "    print(\"Sample overlapping IDs:\", list(overlapping_ids)[:10])\n",
    "else:\n",
    "    print(\"\\nâœ… No duplicate ArticleIds found\")\n",
    "\n",
    "# Calculate leakage percentage\n",
    "id_leakage_percent = (len(overlapping_ids) / len(validation_ids)) * 100\n",
    "print(f\"ID leakage percentage: {id_leakage_percent:.1f}% of validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ DATA LEAKAGE ANALYSIS - TEXT CONTENT\n",
      "==================================================\n",
      "Training set unique texts: 1094\n",
      "Validation set unique texts: 370\n",
      "Overlapping text hashes: 24\n",
      "\n",
      "âš ï¸  CRITICAL: 24 duplicate texts found!\n",
      "\n",
      "Example duplicate articles:\n",
      "  Validation ID 1115 matches Training ID 2098\n",
      "  Text preview: pop band busted to  take a break  chart-topping pop band busted have confirmed that they plan to  ta...\n",
      "\n",
      "  Validation ID 789 matches Training ID 2042\n",
      "  Text preview: virus poses as christmas e-mail security firms are warning about a windows virus disguising itself a...\n",
      "\n",
      "  Validation ID 1937 matches Training ID 636\n",
      "  Text preview: more power to the people says hp the digital revolution is focused on letting people tell and share ...\n",
      "\n",
      "Text leakage percentage: 6.5% of validation set\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate text content (same articles with different IDs)\n",
    "def get_text_hash(text):\n",
    "    \"\"\"Create hash of normalized text for comparison.\"\"\"\n",
    "    # Normalize text: lowercase, strip whitespace, remove extra spaces\n",
    "    normalized = ' '.join(str(text).lower().strip().split())\n",
    "    return hashlib.md5(normalized.encode()).hexdigest()\n",
    "\n",
    "# Create text hashes for both datasets\n",
    "train_data['text_hash'] = train_data['Text'].apply(get_text_hash)\n",
    "validation_data['text_hash'] = validation_data['Text'].apply(get_text_hash)\n",
    "\n",
    "# Find overlapping text hashes\n",
    "train_hashes = set(train_data['text_hash'])\n",
    "validation_hashes = set(validation_data['text_hash'])\n",
    "overlapping_hashes = train_hashes.intersection(validation_hashes)\n",
    "\n",
    "print(\"ðŸš¨ DATA LEAKAGE ANALYSIS - TEXT CONTENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set unique texts: {len(train_hashes)}\")\n",
    "print(f\"Validation set unique texts: {len(validation_hashes)}\")\n",
    "print(f\"Overlapping text hashes: {len(overlapping_hashes)}\")\n",
    "\n",
    "if overlapping_hashes:\n",
    "    print(f\"\\nâš ï¸  CRITICAL: {len(overlapping_hashes)} duplicate texts found!\")\n",
    "    \n",
    "    # Show examples of duplicate texts\n",
    "    duplicate_examples = validation_data[validation_data['text_hash'].isin(overlapping_hashes)]\n",
    "    print(f\"\\nExample duplicate articles:\")\n",
    "    for i, row in duplicate_examples.head(3).iterrows():\n",
    "        matching_train = train_data[train_data['text_hash'] == row['text_hash']]\n",
    "        print(f\"  Validation ID {row['ArticleId']} matches Training ID {matching_train.iloc[0]['ArticleId']}\")\n",
    "        print(f\"  Text preview: {row['Text'][:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nâœ… No duplicate text content found\")\n",
    "\n",
    "# Calculate text leakage percentage\n",
    "text_leakage_percent = (len(overlapping_hashes) / len(validation_hashes)) * 100\n",
    "print(f\"Text leakage percentage: {text_leakage_percent:.1f}% of validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATA LEAKAGE SUMMARY REPORT\n",
      "============================================================\n",
      "Dataset Overview:\n",
      "  Training articles: 1,117\n",
      "  Validation articles: 373\n",
      "  Total articles: 1,490\n",
      "  Train/Val split ratio: 3.0:1\n",
      "\n",
      "ðŸ“‹ ArticleId Leakage:\n",
      "  Duplicate ArticleIds: 0\n",
      "  Leakage rate: 0.0% of validation set\n",
      "\n",
      "ðŸ“ Text Content Leakage:\n",
      "  Duplicate text content: 24\n",
      "  Leakage rate: 6.4% of validation set\n",
      "\n",
      "ðŸŽ¯ OVERALL ASSESSMENT:\n",
      "  ðŸš¨ MODERATE LEAKAGE: 24 articles (6.4%)\n",
      "  âš ï¸  Should remove duplicates to ensure valid evaluation\n",
      "\n",
      "Recommendation:\n",
      "  Remove 24 duplicate articles from validation set\n",
      "  Clean validation set would have 349 articles\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data leakage summary report\n",
    "print(\"ðŸ“Š DATA LEAKAGE SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic stats\n",
    "total_train = len(train_data)\n",
    "total_validation = len(validation_data)\n",
    "total_articles = total_train + total_validation\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"  Training articles: {total_train:,}\")\n",
    "print(f\"  Validation articles: {total_validation:,}\")\n",
    "print(f\"  Total articles: {total_articles:,}\")\n",
    "print(f\"  Train/Val split ratio: {total_train/total_validation:.1f}:1\")\n",
    "\n",
    "# ID-based leakage\n",
    "id_leakage_count = len(overlapping_ids) if 'overlapping_ids' in locals() else 0\n",
    "id_leakage_percent = (id_leakage_count / total_validation) * 100 if total_validation > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“‹ ArticleId Leakage:\")\n",
    "print(f\"  Duplicate ArticleIds: {id_leakage_count}\")\n",
    "print(f\"  Leakage rate: {id_leakage_percent:.1f}% of validation set\")\n",
    "\n",
    "# Text-based leakage\n",
    "text_leakage_count = len(overlapping_hashes) if 'overlapping_hashes' in locals() else 0\n",
    "text_leakage_percent = (text_leakage_count / total_validation) * 100 if total_validation > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“ Text Content Leakage:\")\n",
    "print(f\"  Duplicate text content: {text_leakage_count}\")\n",
    "print(f\"  Leakage rate: {text_leakage_percent:.1f}% of validation set\")\n",
    "\n",
    "# Overall assessment\n",
    "total_leakage = max(id_leakage_count, text_leakage_count)\n",
    "overall_leakage_percent = (total_leakage / total_validation) * 100 if total_validation > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸŽ¯ OVERALL ASSESSMENT:\")\n",
    "if total_leakage == 0:\n",
    "    print(\"  âœ… NO DATA LEAKAGE DETECTED\")\n",
    "    print(\"  âœ… Train/validation split is clean\")\n",
    "elif overall_leakage_percent < 5:\n",
    "    print(f\"  âš ï¸  MINOR LEAKAGE: {total_leakage} articles ({overall_leakage_percent:.1f}%)\")\n",
    "    print(\"  ðŸ“ Consider removing duplicates but may not significantly impact results\")\n",
    "elif overall_leakage_percent < 10:\n",
    "    print(f\"  ðŸš¨ MODERATE LEAKAGE: {total_leakage} articles ({overall_leakage_percent:.1f}%)\")\n",
    "    print(\"  âš ï¸  Should remove duplicates to ensure valid evaluation\")\n",
    "else:\n",
    "    print(f\"  ðŸ”¥ SEVERE LEAKAGE: {total_leakage} articles ({overall_leakage_percent:.1f}%)\")\n",
    "    print(\"  ðŸš¨ MUST remove duplicates - current evaluation is invalid\")\n",
    "\n",
    "print(f\"\\nRecommendation:\")\n",
    "if total_leakage == 0:\n",
    "    print(\"  Proceed with current train/validation split\")\n",
    "else:\n",
    "    print(f\"  Remove {total_leakage} duplicate articles from validation set\")\n",
    "    print(f\"  Clean validation set would have {total_validation - total_leakage} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}